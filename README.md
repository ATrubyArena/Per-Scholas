🏦 Per Scholas Capstone Project
This project was developed as part of the Per Scholas Data Engineering program. The objective was to build a robust ETL data pipeline for a financial institution using PySpark, MySQL, and Python CLI tools. The pipeline performs data extraction, transformation, and loading (ETL), and it includes business intelligence visualizations.

📁 Project Structure
extract(): Reads data from local JSON files and a remote API.

transform(): Cleans and standardizes customer, credit, and branch data using PySpark.

load(): Loads the transformed data into a MySQL relational database.

cli.py: Provides a Command Line Interface to interact with the data (e.g., query by ZIP, SSN, or date range).

visualizations.py: Generates charts to represent key metrics using Matplotlib and Seaborn.

🧰 Tech Stack
Tool	Purpose
Python 3	Core programming language
PySpark	Big data processing and transformation
MySQL	Relational database for storing transformed data
mysql-connector-python	Python library for interacting with MySQL
Matplotlib / Seaborn	Data visualization libraries
Pandas	Used alongside PySpark for some data manipulation
Requests	To access external API data
Rich	Enhanced console output formatting
VS Code	Development environment (recommended)

🔧 Installation
bash
Copy
Edit
# Clone the repository
git clone https://github.com/ATrubyArena/Per-Scholas.git
cd Per-Scholas/Capstone

## ✅ Features

- Ingests data from both local JSON files and an external API
- Cleans and standardizes data using PySpark
- Stores data in a MySQL relational database
- Query customer and transaction data by ZIP code, SSN, or date
- Generates approval rate and transaction visualizations
- Command Line Interface (CLI) for user interaction

## 🛠️ Usage

### Run ETL Pipeline
```bash
python etl.py
Launch CLI Tool
bash

python cli.py
Generate Visualizations
bash

python visualizations.py



yaml
---

### 📊 3. **Example Outputs / Screenshots**
Add images of your CLI in action or visualizations generated by your project (especially helpful for viewers on GitHub).

```markdown
## 📊 Example Visualizations

![Loan Approval Rate](images/loan_approval_chart.png)
![Transaction Volume by Month](images/transaction_volume.png)
🧪 4. Testing
Mention if/how you tested your code (unit tests, manual testing, etc.)

markdown
Copy
Edit
## 🧪 Testing

This project was manually tested using various test cases (e.g., invalid ZIP, missing SSN, edge dates). PySpark transformations were validated with `.show()` and `.describe()` during development.
🗂️ 5. Data Sources
Explain where the data comes from — especially the remote API.

## 🗂️ Data Sources

- `cdw_sapp_customer.json`, `cdw_sapp_credit.json`, `cdw_sapp_branch.json`: Local datasets
- `loan_data.json`: Pulled from [LoanDataset GitHub API](https://github.com/platformps/LoanDataset)
🔐 6. Security Notes (Optional)
Remind users not to hard-code credentials and how to manage them safely.

## 🔐 Security Notes

- Use environment variables or a `.env` file to store database credentials.
- Never upload `.env` or credential files to version control.
🧑‍💻 7. Contributors / Credits
Shout out anyone who helped, including Per Scholas instructors or classmates.


## 👨‍🏫 Credits

This project was developed during the Per Scholas Data Engineering program with guidance from instructors and support from peers.
